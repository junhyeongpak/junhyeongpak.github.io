---
layout: single
title: "Entropy"



---

# Entropy

## 1) 엔트로피

$H(x)=-\sum_{i}^{n}p(x_i)logp(x_i)$

엔트로란 어떤 정보를 표현하는데 필요한 최소 자원량의 기댓값임

- 빈번히 발생하는 사건은 정보량이 적다고 판단하고 빈도가 적은 사건은 정보량이 많다고 판단함
- 예를 들어 키커가 프리킥을 찰 때, 인사이드로 찰 확률이 0.1이고 아웃사이드로 찰 확률이 0.9라 하자 아웃사이드는 너무 당연한 일이라 키퍼에게 새로운 정보가 아니다. 하지만 키커가 인사이드로 차면 상대적으로 더 새로운, 큰 정보량이라고 생각됨
- 따라서 엔트로피가 높다라는 말은 정보량이 많고 정보가 많기 때문에 예측이 불확실하다는 것으로 이해하면 됨
- 위의 예시를 활용하면 각 슛의 스타일이 5:5일 때, 9:1로 슛을 할 때보다 키커가 예측하기 불확실하다는 의미임

---

## 2) 크로스-엔트로피

크로스 엔트로피는 두 확률 분포의 차이를 구하기 위해 사용됨

크로스 엔트로피는 예측 모형은 실제 분포인 q(x)를 따르고, 모델링하여 q분포를 예측하고자 하는 것이다. 예측 모델링을 통해 구한 분포가 p(x)라고 하면 실제 분포인 q를 예측하는 p분포를 만들 때, 크로스 엔트로피는 아래와 같음

$H_p(q)=-\sum_{i=1}^nq(x_i)logp(x_i)$

머신러닝을 통한 예측 모형에서 훈련 데이터는 실제 분포 q를 알 수 있기 때문에 크로스 엔트로피를 계산할 수 있음. 즉, 훈련 데이터를 사용한 예측 모형에서 크로스 엔트로피는 실제값과 예측값의 차이를 계산하는데 사용됨

---

## 3) KL Divergence

$D_{KL}(q||p)=-\sum_{c=1}^Cq(y_c)[log(p(y_c))-log(q(y_c))]=H_p(q)-H(q)$

$D_{kL}(p||q)=\sum_xp(x)log\frac{p(x)}{q(x)}$

KL Divergence는 서로 다른 두 분포의 차이를 측정하는데 쓰이는 측정방법임. 이를 엔트로피와 크로스 엔트로피 개념에 대입하면 두 엔트로피 차이로 계산됨

위의 정의는 p와 q가 이산분포일 때 정의되는 것이며, 연속 분포일 때는 Integral이 들어감

- $H_p(q)$는 실제 엔트로피 값 보다 항상 크기 때문에 KL Divergence는 항상 0보다 큰 값을 갖게 됨
- $H(q)$는 고정이기 때문에, $H_p(q)$를 최소화 시키는 것이 KL Divergence를 최소화 시키는 것이며, 이것이 불확실성을 제어하는 예측모형의 실질적 목적임