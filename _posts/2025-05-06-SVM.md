---
layout: single
title: "Support Vector Machines"


---

# SVM

- 두 가지 종류의 데이터의 집합이 주어졌을 때, 이를 기반으로 새로운 입력 데이터가 둘 중 어느 카테고리에 속할지 판단하는 비확률적 이진 선형 분류 모델
- 커널트릭(Kernel Trick)을 활용하여 비선형 분류 문제에도 사용 가능

## SVM의 이해

- Data: $\{(x_i,y_i)\}^{m}_{i=1}$

- 분리 초평면(Sparating Hyperplane):

  - 서로 다른 클래스를 완벽하게 분류하는 기준

    |   차원   | 공간을 둘로 나누는 도형 |
    | :------: | :---------------------: |
    |  2차원   |       직선(1차원)       |
    |  3차원   |       평면(2차원)       |
    | $\vdots$ |        $\vdots$         |
    |  n차원   |     초평면(n-1차원)     |

  - 최대 마진 초평면(Maximum Margin Hyperplane)

    - 두 클래스 집단 사이의 거리 폭 최대로 함

  - 서포트 벡터(Support Vectors)

    - 결정 경계선에 가장 가까이에 있는 각 클래스의 데이터

  - Hyperplanes = 다변수 일차방정식
    $$
    \theta \, \cdot x\, +\theta_0=0
    $$

  - $|\theta\, \cdot \, x + \theta_0|=1$, Canonical Hyperplane

  - margin의 크기
    $$
    m=\frac{1}{||\theta||}
    $$

  - SVM의 제한조건
    $$
    y_i(\theta \, \cdot x_i \, +\theta_0 \ge1)
    $$

### SVM의 최적화 문제

- Original Problem: margin의 최대화
  $$
  \underset{\theta, \theta_0}{min}\frac{||\theta||}{2}
  \\subject\, to \, y_i(\theta \, \cdot x_i \, + \theta_0) \ge1, i=1,2,\cdots, m
  $$
  Question) 최적화 문제에서 해의 범위 정보(constraints)가 존재한다면?

  -> Duality(쌍대성)를 이용하면 주어진 최적화 문제를 해결이 용이한 문제로 변환이 가능하며, 최적화 문제의 해에 대한 하한 또는 상한을 추측할 수 있음

---

## Duality Principle (쌍대성 원칙)

- 최적화 이론에서, 어떤 최적화 문제가 원초 문제(Primal problem) 또는 상대문제(Dual problem) 두 가지 관점에서 해석될 수 있는 원칙을 의미

  - 이때 쌍대문제의 상한은 원초문제의 하안이 됨

    ![img](/Users/wnsgud/blog/junhyeongpak.github.io/img/duality principle weak.png)
  
  ---
  
  ### Primal Problem
  
  - $x, \theta \in \mathbb{R}^n, \, b \in \mathbb{R}^m, \, d \in \mathbb{R}^p \, A \in \mathbb{R}^{m\times n}, C \in \mathbb{R}^{p\times n} $ for linear objective function with linear constraints as:
    $$
    minimize \, \theta^Tx \\ subject \, to \, Ax \le b, \\ Cx =d
    $$
  
    - 부등식과 등식형태의 제약식 양변에 $\lambda, v$라는 벡터를 곱해보자. 부등식의 방향이 바뀌지 않도록 벡터 $\lambda$의 원소는 모두 0이상의 양수 값을 갖는다.
      $$
      \lambda^TAx\le\lambda^Tb, \quad v^TCx=v^Td \quad 에서 두식을 더하고 정리하면
      \\ \lambda^TAx\, +v^TCx \le \lambda^Tb \, + v^Td
      \\ (A^T\lambda \, + C^Tv)^Tx \le \lambda^Tb \, + v^Td
      \\(-A^T\lambda \, - C^Tv)^Tx \, \ge -\lambda^Tb\, -v^Td
      $$
      -> Primal problem의 목적함수 $\theta^Tx$의 하한: $-\lambda^Tb\, -v^Td$
  
  ### Dual Problem
  
  - $b \in \mathbb{R}^m, \, d \in \mathbb{R}^p \, A \in \mathbb{R}^{m\times n}, C \in \mathbb{R}^{p\times n}$ for linear objective function with linear constraints as:
    $$
    \underset{\lambda,v}{max}-b^T\lambda \, -d^Tv
    \\ subject \, to \quad -A^T\lambda \, -C^Tv=\theta
    \\ \lambda \ge0
    $$
  
  ### Convex Optimazation Problem
  
  - A convex optimazation problem with variables x:
    $$
    minimize \quad f_0(x)
    \\subject \, to \quad f_i(x) \le 0, \quad i=1, 2, \cdots,m
    \\h_i(x)=0, \quad i=1,2,\cdots,p
    \\where \, f_0,f_1, \cdots,f_m \, are \,convex\, functions.
    $$
  
    - convex 목적 함수를 최소화 하거나 concave 목적함수를 최대화
    - convex 함수에서 상한 부등식 제약조건은 convex해야됨
    - 등식 제약조건은 반드시 affine해야 함
  
  ---
  
  ### 제약조건이 없는 문제
  
  - 지시함수(indicator function)를 활용해서 제약조건을 식에 포함시킴
  
  - 다만, 지시함수를 사용하면 함수 값이 $\infin$를 포함하고 함수가 불연속해지기 때문에 지시힘수를 선형함수로 근사시키는 방법을 사용: $\delta_1(x): \, \lambda_ix \,\,(\lambda_i\ge0), \,\delta_2(x):\, v_ix$
    $$
    minimize \quad f_0(x)\, +\Sigma_{i=1}^m\lambda_if_i(x)\,+\Sigma_{i=1}^pv_ih_i(x)
    $$
  
  - 위 접근방식을 Lagrangian Approach 즉, 라그랑주 접근법이라 일컫는다
  
  ---
  
  ### 라그랑주 함수 정의
  
  - 벡터 $\lambda=(\lambda_1,\cdots,\lambda_m)\in \mathbb{R}^m,v=(v_1,\cdots,v_p)\in\mathbb{R}^p, x=(x_1,\cdots,x_n)\in\mathbb{R}^n$에 대하여, 아래 최적화 문제의 라그랑주 함수 $L: \mathbb{R}^n\times\mathbb{R}^m\times\mathbb{R}^p\rarr \mathbb{R}$은 다음과 같이 정의함
    $$
    L(x, \lambda, v)=f(x)\,+\Sigma_{i=1}^m\lambda_if_i(x)\,+\Sigma_{i=1}^pv_ih_i(x)
    $$
  
  - 이때 벡터 $\lambda, v$는 라그랑주 승수 또는 쌍대변수라고 일컫는다.
  
  ---
  
  ### 라그랑주 쌍대함수 정의
  
  - 라그랑주 함수 $L: \mathbb{R}^n\times\mathbb{R}^m\times\mathbb{R}^p\rarr \mathbb{R}$에 대하여, 라그랑주 쌍대 함수 $g: \mathbb{R}^m\times\mathbb{R}^p\rarr\mathbb{R}$는 다음과 같이 $x \in \mathbb{R}^n$에 대한 라그랑주 함수 $L$의 하한으로 정의함
    $$
    g(\lambda, v)=inf_{x\in \mathbb{R}^n}L(x, \lambda, v)=inf_{x\in \mathbb{R}^n}(f(x)\,+\Sigma_{i=1}^m\lambda_if_i(x)\,+\Sigma_{i=1}^pv_ih_i(x))
    $$
  
  - 주어진 라그랑주 함수의 최소값을 라그랑주 승수로 표현함
  
  - $g(\lambda,v)=\underset{x}{min}L(x,\lambda,v)$
  
  - Lagrange Dual Problem:
    $$
    \underset{\lambda, v}{max}\, g(\lambda,v)
    \\ subject \, to \quad \lambda \ge 0
    $$
  
  ### 라그랑주 승수법
  
  - 목적함수에 라그랑주 승수 항을 더하여 제약이 있는 최적화 문제를 제약이 없는 문제로 바꾸는 방법
    $$
    \underset{x}{min}\quad \theta^Tx
    \\subject \, to \,  Ax \le b
    \\\quad Cx =d
    \\\rarr L(x, \lambda, v)=\theta^Tx\,+\lambda^T(Ax-b)\, +v^T(Cx-d)\le\theta^Tx
    $$
  
  ### 라그랑주 쌍대 문제
  
  - 라그랑주 쌍대 함수인 $g(\lambda, v)$는 정의에 의해 라그랑주 함수 $L(x, \lambda, v)$의 최소값
  
    - $L(x, \lambda, v)$는 편미분 결과가 0이 되는 지점에서 최소값을 가짐
      $$
      \underset{\lambda, v}{max}\quad -\lambda^Tb-v^Td
      \\subject \, to \, -A^T\lambda-C^Tv=\theta
      \\\quad \lambda \ge 0
      $$
  
  ### KKT(Karush-Kuhn-Tucker) Conditions
  
  - Given General problem:
    $$
    minimize \quad f_0(x)
    \\ subject\, to \quad f_i(x) \le 0, \quad i=1, 2, \cdots, m\rarr inequlity \, constraints
    \\ \quad\quad h_i(x) = 0, \, \quad i=1, 2, \cdots,p\rarr equality \, constraints
    $$
  
  - KKT에 대한 필요충분 조건
  
    - Stationarity
  
      $0\in\sigma(f(x)\,+\Sigma_{i=1}^m\lambda_if_i(x)\, +\Sigma_{i=1}^pv_ih_i(x))$
  
    - Complementary slackness
  
      $\lambda_if_i(x)=0$ for all i
  
    - Primal feasibility
  
      $f_i(x)\le0, \, h_i(x)=0$ for all i
  
    - Dual feasibility
  
      $\lambda_i\ge0$ for all i

## SVM의 최적화 문제 - Duality

- Original Problem: margin의 최대화
  $$
  \underset{\theta,\theta_0}{min}\frac{||\theta||}{2}\approx\underset{\theta,\theta_0}{min}\frac{||\theta||^2}{2}
  \\ subject\,to\, y_i(\theta^Tx_i\,+\theta_0) \ge1, \,\,i=1, 2,\cdots,m
  $$

- Primal Problem: 라그랑지 승수 해법으로 해결

  - 제약식을 목적식에 포함
    $$
    \underset{\theta, \theta_0}{min}=\frac{||\theta||^2}{2}+\Sigma_{i=1}^m\lambda_i(1-y_i(\theta\cdot x_i+\theta_0))
    \\=\frac{||\theta||^2}{2}-\Sigma_{i=1}^m\lambda_i(y_i(\theta\cdot x_i+\theta_0)-1)
    $$
    -> 위에서 $1-y_i(\theta\cdot x_i+\theta_0)$가 primal feasibility를 만족

    또한 $\lambda_i(1-y_i(\theta\cdot x_i+\theta_0))=0\, and\, \lambda_i\ge0$ for all $i$이므로 complement slacknessd와 dual feasibility를 만족

    다음으로 $\theta$와 $\theta_0$에 대해 편미분한 제약조건에 대해서 정리하면
    $$
    \underset{\lambda}{max}(\theta,\theta_0,\lambda)=\Sigma_{i=1}^m\lambda_i-\frac{1}{2}\Sigma_{i=1}^m\Sigma_{j=1}^m\lambda_i\lambda_jy_iy_jx_i^Tx_j
    \\ subject \, to\, \lambda_i\ge0, \, i=1, 2, \cdots,m
    \\ \Sigma_{i=1}^m\lambda_iy_i=0
    $$

  - Complementary slackness

    - $x_i$가 서포트 벡터가 아닌 경우 결정경계에 영향을 미치지 않음 outlier에 강함

---

## 선형 SVM

- Hard Margin SVM
  - 완벽하게 선형 분리가 가능한 문제 $\rarr$ 비현실적
- Soft Margin SVM
  - 선형 분리가 불가능한 문제
  - 학습 데이터의 에러가 0이 되도록 완벽하게 분리하는 것은 불가능
  - 오차를 허용 slack variable $\xi$ for $x_i$ ($\xi_i\ge0$)
  - $\xi = 0$ $\rightarrow$ 마진에 벡터가 존재
  - $0 < \xi_i < 1$ $\rightarrow$ 마진 내에 벡터가 존재
  - $\xi_i > 1$ $\rightarrow$ 오분류


---

### Soft Margin Classifier

- 에러를 허용하면서도 margin을 최대화 하는 것
  $$
  \underset{\theta,\theta_0,\xi_i}{min}\frac{||\theta||^2}{2}+c\Sigma_{i=1}^m(\xi_i)^k
  \\subject \, to \, y_i(\theta^Tx_i +\theta_0)\ge1-\xi_i, \, \xi \ge 0, \, i=1,2,\cdots,m
  $$

- Hinge Loss(k=1) vs Quadratic Loss(k=2)

  - Hinge Loss같은 경우 제약 조건을 두개를 추가해서 라그랑주 상수를 활용하면 됨
    1. $y_i(\theta^Tx_i+\theta_0)\ge1-\xi_i$
    2. $\xi_i\ge0$

  $\rightarrow$ $0 \le \lambda_i \le C$로 바뀐 것 말고는 Hard Margin Classifiier와 동일

---

## 비선형 SVM

- Feature Expansion

  - 데이터를 선형으로 분류하기 위해 차원을 높이는 방법을 사용
  - Feature Map($\Phi$)을 통해 차원을 높임
  - SVM 모델 Original Space가 아닌  Feature Space에서 학습
  - [문제점]: 시간 및 공간 복잡도가 기하학적으로 급증

- Kernel Trick

  - Feature Space를 내적으로 표현 가능한 경우에는 Feature expansion이 필요없이 내적결과를 얻을 수 있음
    $$
    \underset{\lambda}{max}L_{dual}=\Sigma_{i=1}^m\lambda_i-\frac{1}{2}\Sigma_{i=1}^m\Sigma_{j=1}^m\lambda_i\lambda_jy_iy_jK(x_i,xj)
    \\ subject\,to\,0\le\lambda_i\le C, \, \Sigma_{i=1}^m\lambda_iy_i=0, \, i=1, 2, \cdots,m
    $$

  - Kernel trick을 이용해서 kernel 매핑 과정없이 내적 결과를 바로 계산 가능하므로 연산 효율이 증가

  - 대표적으로 Linear Kernel, Polynomial Kernel, Sigmoid Kernel, RBF Kernel이 존재

- RBF(Gaussian Radial Basis Function Kernek)

  - $K(x, z)=exp(-\gamma||x-z||^2_2)$
  - $\gamma$: Gaussian kernel 폭의 역수에 해당 ($\gamma \propto \frac{1}{\sigma}$)
    - 하나의 학습 데이터 샘플이 미치는 영향의 범위 결정
      - $\gamma$값이 작을수록 넓은 영역을 의미하고 하나의 샘플이 미치는 영향이 제한(과소적합)
      - $\gamma$값이 클수록 좁은 영역을 의미하고 하나의 샘플이 미치는 영향이 커짐(과적합)

- 커널 선정 방법

  - 사용하는 kernel에 따라 feature space가 달라지므로 데이터의 특성에 맞는 kernel 선정이 중요
  - 일반적으로 RBF, sigmoid, low degree polynomial(4차 미만) kernel이 주로 사용됨
