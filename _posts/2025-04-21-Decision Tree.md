---
layout: single
title: "Decision Treet"

---

# 의사결정 나무

## 의사결정나무

- 학습 데이터를 분석하여 데이터에 내재되어 있는 패턴을 통해, 한번에 하나씩의 설명변수를 사용하여 분류와 회귀가 가능한 규칙들의 집합을 생성하는 모델
- '스무고개'와 비슷
  - 목적(Y)과 자료(X)에 따라 적절한 분리 기준(질문)과 정지 규칙(ex: 트리의 깊이)를 지정하여 의사결정나무를 생성
- 의사결정방식의 과정의 표현법이 '나무 '와 같다고 해서 의사결정나무라고 불림

### 의사결정나무의 장점

- 이해하기 쉽고 적용이 쉬움

  - 나무 구조(if-then 규칙)에 의해 표현하기 때문에 모델을 쉽게 이해 가능

- 중요한 변수 선택에 유용(상단에서 사용된 설명 변수가 중요한 변수)

  - 변별력이 높은 결정적 질문에 해당

- 데이터의 통계적 가정이 필요 없음(ex: LDA - 데이터 정규분포 가정)

- 의사결정과정에 대한 설명(해석) 가능

  ![image-20250420164430412](/Users/wnsgud/Library/Application Support/typora-user-images/image-20250420164430412.png)

- 의사결정나무 특성

  - 전체적으로 보면 나무를 뒤집어 놓은 것과 비슷
  - Root node에서 분기가 거듭될 수록 그에 해당하는 데이터 개수는 감소
  - 각 leaf/terminal 노드에 속하는 데이터 수의 총합은 root node의 데이터 수와 일치
    - 즉, leaf node 간 교집합이 없음

- 의사결정나무 단점

  - 많은 데이터가 필요

  - 시간 비용이 큼

  - 데이터의 변화에 민감(도메인 갭이 적어야 함)

  - 선형 구조 형 데이터 예측시 더 복잡해짐

    ![image-20250420164707280](/Users/wnsgud/Library/Application Support/typora-user-images/image-20250420164707280.png)

## 의사결정나무 - 데이터분석 절차

- [순서]: 데이터 -> 모델학습 -> 추론
- 데이터: 다변량(Multivairate Variable) 사용
- 모델학습(트리구조 이용):
  1. 한번에 설명 변수 하나씩 데이터 선택
  2. 2개 혹은 그 이상의 부분집합으로 분할
  3. 데이터 순도가 균일해지는 방향으로 Recursive Partitioning(재귀적 분할)
     - [재귀적 분할 종료 조건]:
       - 분류 문제: 끝 노드에 비슷한 범주를 갖고 있는 관측데이터끼리
       - 예측 문제: 끝 노드에 비슷한 수치를 갖고 있는 관측데이터끼리
- 추론(판별):
  - 분류: 끝 노드에서 가장 빈도가 높은 범주(y)에 새로운 데이터를 분류
  - 회귀: 끝 노드의 종속변수(y)의 예측 값으로 반환

## 의사결정나무 - 분류와 회귀

- 의사결정나무는 분류와 회귀가 모두 가능
  - 분류 나무: 목표 변수가 범주형 -> 분류
  - 회귀 나무: 목표 변수가 연속형 수치 -> 예측
- 분할 기준: 불순도/불확실성
  - 지니 계수
  - 엔트로피/정보 이득/정보 이익 비율
  - 카이제곱 통계량
- 재귀적 분할 알고리즘
  - CART
  - C4.5, C5.0
  - CHAID

## 의사결정나무의 분할기준 이해

의사결정나무 분기 시의 변수의 영역 분할기준

- 순도(purity): homogeneity, 균질성 증가
- 불순도(impurity), 불확실성(uncertainty) 감소

### 정보량(Information for an event)

$P(x)$는 어떤 event $x$가 일어날 확률이고, 이 사건(event)가 가지고 있는 정보의 양(Information $I(x)$는 다음과 같이 표현됨

$I(x)=-log_2\,P(x)=log_2\frac{1}{P(x)}$

- Base-2 bit logarithm: 정보 측정의 단위가 비트
  - 예를 들어 $P(x)=\frac{1}{4}:\,I(x)=2bit$
- 음수 표현: 정보의 양이 항상 0이상의 양수가 되도록 함

### Entropy

- 랜덤변수의 정보량을 수치화
- 랜덤변수 $X$의 값($k$)는 어떤 확률분포를 따르며, 사건 $(X=k)$를 표현할 수 있는 평균 정보의 양
  - $H(x)=-\underset{k}{\Sigma}P(x=k)\,log_2P(X=k)=-\underset{k}{\Sigma}P(k)\,log_2P(k)$


### Cross-Entropy

- 하나의 확률 분포를 다른 확률분포로 인코딩할 때 필요한 평균 정보량
  - $H(P, Q)=-\underset{k}{\Sigma}\,P(k)\,log_2Q(k)$
  - P: 실제 확률분포, Q: 모델의 예측 확률 분포
- 즉, 모델 Q가 실제 데이터 P를 설명하는데 필요한 평균 정보량

### KL-Divergence(Kullback-Leibler Divergence)

- 두 확률 분포 간의 비대칭적 거리
  - Q가 P를 얼마나 설명하지 못하는가?
  - $D_{KL}(P||Q)=\underset{k}{\Sigma}P(k)\frac{P(k)}{Q(k)}\neq D_{KL}(Q||P)$

## 의사결정나무 - 분할기준(엔트로피)

- 엔트로피 - 한 개 영역
  - Entropy(A): m개의 클래스가 속하는 A영역에 대한 엔트로피
  - $p_k$: A영역에 속하는 레코드 가운데 k 범주에 속하는 레코드의 비율
  - $Entropy(A)=-\underset{k=1}{\overset{m}{\Sigma}}p_k\,log_2(p_k)$
- 엔트로피 - 두 개 이상의 영역
  - Entropy(A): m개의 클래스가 속하는 A의 분할 영역(d)에 대한 엔트로피
  - $R_i$: 분할 전 레코드 중에서, 분할 후 i 영역에 속하는 레코드의 비율
  - $Entropy(A)=\underset{i=1}{\overset{d}{\Sigma}}(R_i(-\underset{k=1}{\overset{k}{\Sigma}}p_{ik}\,log_2{p_{(ik)}}))$
- 엔트로피 감소의 의미
  - 불확실성 감소 -> 순도 증가
  - 즉, 의사결정나무 모델은 분할 후가 분할 전보다 불확실성이 감소하므로 분할을 결정

### Gini Index

- 데이터의 통계적 분산정도를 정량화해서 표현한 값
- 정답이 아닌 다른 라벨이 선택될 확률 -> 이게 왜 분산일까 생각해보면... 한쪽 정답으로 몰리게 되면 분산이 작아지고 여러 개로 나뉘면 분산이 커지기 때문임
- $GI(A)=1-\underset{k=1}{\overset{m}{\Sigma}}p_k^2$
- 지니계수도 엔트로피와 마찬가지로 한 개의 영역과 두 개 이상의 영역에 적용되는 영역별 지니계수 산출 방식이 동일함

## 의사결정나무 - 분류나무

- 목표변수: 범주형 변수
- 분류 알고리즘과 불순도 지표
  - CART: 지니 지수
  - ID3, C4.5, C5: 엔트로피 기반
    - ID3: 정보 이익, C4.5와 C5: 정보 이득 비율
  - CHAID: 카이 제곱 통계량, 통계적 유의성 검정을 활용해 분할 결정

### 분류나무 - CART(Classification And Regression Tree) 알고리즘

- 가장 많이 사용됨
- [종류]: 분류 나무, 회귀 나무
- [분리]: 이진 분할
- [불순도 알고리즘]: Gini Index는 낮아지는 것이 좋음

### 분류나무(Classification Tree): ID3

- [분리]: 다중 분할
- [불순도 알고리즘]: 엔트로피 기반 정보 이익
- 정보 이론 -> 엔트로피
- 정보 이익: 정보의 가치가 높아져야 좋음
  - $IG(Information Gain)$
  - $IG = E(before)-E(after)$

### 분류나무(Classification Tree): C4.5 및 C5.0

- 정보 이득율(Information Gain Ratio)
  - C4.5에서는 정보 이득율을 추가적으로 도입
  - 가지수가 많아질수록 정보이득이 높아지는 경향을 보임
  - 데이터를 너무 잘게 분해하는 것은 좋은 지표라고 볼 수 없으므로 정보 이득에는 한계점이 있음
  - 이런 단점을 보안하기 위해 IV(Intrinsic Value)를 도입해 정규화 하고자 함
    - 즉, 가지가 많으면 감점
    - 특정 지표로 분기할 때, 생성되는 가지수를 $n$, $i$번째 가지에 해당하는 확률을 $p_i$라 할 때,
    - $IV(A)=-\underset{i}{\overset{n}{\Sigma}}p_i\,log_2(p_i)$

## 분류나무 과적합 방지

- Stop Condition
  - 특정 시점에 모델의 성장을 멈춤
    - 노드 내의 최소 관측수 설정
    - 리프노드의 최대 개수 지정
    - 리프 노드가 가질 수 있는 최소 샘플 수 제한
    - 불순도 최소 감소
    - 분할 시 적용되는 최대 feature 수 지정
- Pruning
  - 완전 모형 생성 후 가지치기
  - 기본적으로 성장 멈추기보다 성능 우수
  - 가지치기 비용함수를 최소로하는 분기를 찾음
- Pruning 단계
  - [Step 1]: 데이터를 training/pruning/test 데이터로 분류
  - [Step 2]: 학습데이터로 의사결정 나무 생성
  - [Step 3]: pruning data로 가지치기 수행
    - $Cost(T) = Err(T) + \alpha \times L(T)$ 
      - $Err(T)$: 오분류율
      - $L(T)$: 리프 노드의 수
  - [Step 4]: 가지치기가끝난 의사결정나무에 시험데이터로 결과 예측

## 의사결정나무 - 회귀나무

- 목표변수: 수치형 변수
- 회귀 결과(입력 데이터의 결과 예측)
  - 끝 노드 데이터들의 평균으로 결정
  - 예측일 경우 회귀나무보다 신경망 또는 회귀분석이 좋음
- 불순도 측정 방법
  - SSE(Sum of the Squared Errors)
  - 오차 = 실제값 - 예측값
- 평가 지표: RMSE

## 의사결정나무 - 앙상블(Ensemble) 기법 적용

- 앙상블이란?
  - 다수의 표본, 다수의 모델에서 나온 결과를 다수결(Voting)로 결정하는 방식
- 랜덤 포레스트
  - 여러 개의 Weak Learner를 결합하면 Single Learner보다 더 좋은 성능을 낼 수 있다는 가정하에 제안된 Decision Tree 기반 모델
  - Single Learner: 전체 데이터를 학습한 결정 트리
  - Weak Learner: 임의의 일부를 학습한 결정 트리
- 부트 스트랩(Boot Strap)
  - 중복을 허용한 샘플링 - 전체 데이터로부터 무작위 복원 추출하여 여러 개 학습 데이터 샘플 추출
  - 랜덤 포레스트는 원본 데이터셋과 같은 크기의 부트스트래핑 샘플을 사용해 각 트리를 생성
- Bagging
  - 부트스트랩 단계 후에 각각 의사결정나무는 자신만의 결과 도출
  - 결과 결합: (분류 문제) -> 투표, (예측 문제) -> 평균
- 랜덤포레스트 기법 특징
  - 개별 의사결정나무는 부트스트래핑 샘플을 활용하므로 다양성이 높아지고, 이를 집계하면 분산이 줄어드는 효과를 볼 수 있음
  - 비록 나무 구조이지만, 숲이 되면서 해석 가능한 모델 장점이 사라짐
  - 하지만, 결과 분석을 통해 설명 변수 중 중요한 변수 판별이 가능



